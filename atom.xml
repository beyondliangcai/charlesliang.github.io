<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Charles</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-16T13:34:38.288Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Charles</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LDA数学基础：Monte Carlo Method</title>
    <link href="http://yoursite.com/2019/03/16/Monte-Carlo/"/>
    <id>http://yoursite.com/2019/03/16/Monte-Carlo/</id>
    <published>2019-03-16T13:24:17.000Z</published>
    <updated>2019-03-16T13:34:38.288Z</updated>
    
    <content type="html"><![CDATA[<!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script> --><h2 id="Monte-Carlo-Method"><a href="#Monte-Carlo-Method" class="headerlink" title="Monte Carlo Method"></a>Monte Carlo Method</h2><blockquote><p>主要了参考刘建平Pinard老师的文章和LDA 数学八卦.pdf ，有兴趣的同学可以去深入学习一下：<a href="https://www.cnblogs.com/pinard/p/6625739.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6625739.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; --&gt;
&lt;h2 id=&quot;Monte-Carlo-
      
    
    </summary>
    
    
      <category term="LDA,Markov,Monte Carlo]" scheme="http://yoursite.com/tags/LDA-Markov-Monte-Carlo/"/>
    
  </entry>
  
  <entry>
    <title>机器学习分类算法一：逻辑回归（logistic regression）</title>
    <link href="http://yoursite.com/2018/09/07/logistic/"/>
    <id>http://yoursite.com/2018/09/07/logistic/</id>
    <published>2018-09-06T16:19:32.000Z</published>
    <updated>2019-03-16T13:20:40.948Z</updated>
    
    <content type="html"><![CDATA[<!-- <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script> --><h2 id="逻辑回归（logistic-regression）"><a href="#逻辑回归（logistic-regression）" class="headerlink" title="逻辑回归（logistic regression）"></a>逻辑回归（logistic regression）</h2><blockquote><p>机器学习主要解决两个方面的问题：分类和聚类，或者叫监督学习和非监督学习。所谓分类通俗来讲指的是训练集中的样本知道y的值（也就是结果），例如一个西瓜的例子：现在想训练一个模型f来分类西瓜甜还是不甜，我们就找一批西瓜（训练集），然后我们就观察这批西瓜，发现（根茎大，皮绿，甜），那么根茎，皮就是特征X，甜和不甜就是分类的结果Y，因为训练集里面知道了Y的值，这种就是分类，如果我们不知道Y的值，只想把根茎大，皮绿的西瓜放到一起，其他的放到另一边，这种就是聚类。对于机器学习来说，如何找到模型f是关键问题，逻辑回归就是其中一种比较简单的线性模型。</p></blockquote><h3 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h3><p>我们高中或者大学可能已经学过了最小二乘法，这里简单的复习一下，最小二乘法也是逻辑回归的基础，先直观感受一下最小二乘法，如下图所示绿色的点表示训练集，红色的线是我们用最小二乘法回归出来的一条直线，由此可见最小二乘法就是要找一条直线使得这条直线能“更好“的反映训练集。<br>什么样的回归直线才是最好的呢？就是每个点到这条直线的欧式距离最短直线就是最好的直线。<br>欧式距离的定义：<br>$$ \rho=\sqrt{(x_2-x_1)^2+{(y_2-y_1)^2}}$$<br>通俗的讲欧式距离就是两个点坐标的平方差。<br><img src="/2018/09/07/logistic/regression.png" alt="Alt text" title="least_square"><br>python 源码如下，需要安装scipy和matplotlib<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 18-9-6 下午9:58</span></span><br><span class="line"><span class="meta">@Author  : Shark</span></span><br><span class="line"><span class="comment"># @Site    :</span></span><br><span class="line"><span class="comment"># @File    : least_squar.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##最小二乘法</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np   <span class="comment">##科学计算库</span></span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp   <span class="comment">##在numpy基础上实现的部分算法库</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment">##绘图库</span></span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> leastsq  <span class="comment">##引入最小二乘法算法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">     设置样本数据，真实数据需要在这里处理</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment">##样本数据(Xi,Yi)，需要转换成数组(列表)形式</span></span><br><span class="line">Xi=np.array([<span class="number">6.19</span>,<span class="number">2.51</span>,<span class="number">7.29</span>,<span class="number">7.01</span>,<span class="number">5.7</span>,<span class="number">2.66</span>,<span class="number">3.98</span>,<span class="number">2.5</span>,<span class="number">9.1</span>,<span class="number">4.2</span>])</span><br><span class="line">Yi=np.array([<span class="number">5.25</span>,<span class="number">2.83</span>,<span class="number">6.41</span>,<span class="number">6.71</span>,<span class="number">5.1</span>,<span class="number">4.23</span>,<span class="number">5.05</span>,<span class="number">1.98</span>,<span class="number">10.5</span>,<span class="number">6.3</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    设定拟合函数和偏差函数</span></span><br><span class="line"><span class="string">    函数的形状确定过程：</span></span><br><span class="line"><span class="string">    1.先画样本图像</span></span><br><span class="line"><span class="string">    2.根据样本图像大致形状确定函数形式(直线、抛物线、正弦余弦等)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##需要拟合的函数func :指定函数的形状</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(p,x)</span>:</span></span><br><span class="line">    k,b=p</span><br><span class="line">    <span class="keyword">return</span> k*x+b</span><br><span class="line"></span><br><span class="line"><span class="comment">##偏差函数：x,y都是列表:这里的x,y更上面的Xi,Yi中是一一对应的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(p,x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> func(p,x)-y</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">    主要部分：附带部分说明</span></span><br><span class="line"><span class="string">    1.leastsq函数的返回值tuple，第一个元素是求解结果，第二个是求解的代价值(个人理解)</span></span><br><span class="line"><span class="string">    2.官网的原话（第二个值）：Value of the cost function at the solution</span></span><br><span class="line"><span class="string">    3.实例：Para=&gt;(array([ 0.61349535,  1.79409255]), 3)</span></span><br><span class="line"><span class="string">    4.返回值元组中第一个值的数量跟需要求解的参数的数量一致</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#k,b的初始值，可以任意设定,经过几次试验，发现p0的值会影响cost的值：Para[1]</span></span><br><span class="line">p0=[<span class="number">1</span>,<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#把error函数中除了p0以外的参数打包到args中(使用要求)</span></span><br><span class="line">Para=leastsq(error,p0,args=(Xi,Yi))</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取结果</span></span><br><span class="line">k,b=Para[<span class="number">0</span>]</span><br><span class="line">print(<span class="string">"k="</span>,k,<span class="string">"b="</span>,b)</span><br><span class="line">print(<span class="string">"cost："</span>+str(Para[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">"求解的拟合直线为:"</span>)</span><br><span class="line">print(<span class="string">"y="</span>+str(round(k,<span class="number">2</span>))+<span class="string">"x+"</span>+str(round(b,<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">   绘图，看拟合效果.</span></span><br><span class="line"><span class="string">   matplotlib默认不支持中文，label先用英文表示</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画样本点</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>)) <span class="comment">##指定图像比例： 8：6</span></span><br><span class="line">plt.scatter(Xi,Yi,color=<span class="string">"green"</span>,label=<span class="string">"data"</span>,linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画拟合直线</span></span><br><span class="line">x=np.linspace(<span class="number">0</span>,<span class="number">12</span>,<span class="number">100</span>) <span class="comment">##在0-15直接画100个连续点</span></span><br><span class="line">y=k*x+b <span class="comment">##函数式</span></span><br><span class="line">plt.plot(x,y,color=<span class="string">"red"</span>,label=<span class="string">"regression"</span>,linewidth=<span class="number">2</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>) <span class="comment">#绘制图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>接下来，我们详细推导一下最小二乘法：<br>假设我们回归出来的直线，用下面公式表示：<br>$$ŷ = \vec{w}x+b ;w=(w_1,w_2,…,w_n)$$<br>有些书上可能用另一种方式表示：我们也可把$w_0$看成b，那么公式可以简写为：<br>$$ŷ = \vec{w}x ;w=(w_0,w_2,…,w_n)$$<br>下面以一元函数举例，公式字母含义ŷ 是回归预测出的值，$\vec{w}$是直线的斜率。因为回归出的直线是最好的，所以样本中的每个点（x,y）到(x,ŷ)的欧式距离最小，所以我们使得损失函数L(x)最小：<br>$$ L(x)=\sum_{x}(y-ŷ)^2$$<br>带入可得：<br>$$ L(w,b)=\sum_{i=1}^n(y_i-(wx+b))^2$$<br>$$=\sum_{i=1}^n(y_i^2-2(wx+b)y_i+(wx+b)^2)1^2<br>$$<br>$$ =y_1^2-2y_1wx_1-2y_1b+w^2x_1^2+2wx_1b+b^2+…+y_n^2-2y_nwx_n-2y_nb+w^2x_n^2+2wx_nb+b^2$$<br>$$ = \sum_{i=1}y_1^2-2w\sum_{i=1}x_iy_i-2b\sum_{i=1}y_i+w^2\sum_{i=1}x_i^2+2wb\sum_{i=1}x_i+nb^2$$</p><p>标记：$ \overline{y^2}=\frac{\sum{i=1}y_i^2}{n} $<br>于是：<br>$$ L(w,b)=n\overline{y^2} -2wn\overline{xy}-2bn\overline{y}+w^2n\overline{x^2}+2wbn\overline{x}+nb^2 $$<br>分别对w,b 求偏倒导：<br>$$ \frac{\partial L}{\partial w}=-2n\overline{xy}+2wn\overline{x^2} +2bn\overline{x} $$<br>$$ \frac{\partial L}{\partial b}=-2n\overline{y}+2wn\overline{x} +2bn$$<br>所以得出w,b<br>$$ w=\frac{\overline{x}*\overline{y}-\overline{xy}}{\overline{x}^2-\overline{x^2}}$$<br>$$ b=\overline{y}-w\overline{x}$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; --&gt;
&lt;h2 id=&quot;逻辑回归（logisti
      
    
    </summary>
    
      <category term="classification" scheme="http://yoursite.com/categories/classification/"/>
    
    
      <category term="最小二乘" scheme="http://yoursite.com/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>charles blog menu</title>
    <link href="http://yoursite.com/2018/09/06/menu/"/>
    <id>http://yoursite.com/2018/09/06/menu/</id>
    <published>2018-09-06T12:19:32.000Z</published>
    <updated>2018-09-06T13:49:00.866Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在学习AI相关的知识，学了这么长时间总感觉前面的东西又忘了，而且我觉得只要写不出来的算法都是没有搞懂的，所以从今天开始写博客,复习学过的AI相关的知识，大概先从以下三个方向写：</p></blockquote><h2 id="1-机器学习篇"><a href="#1-机器学习篇" class="headerlink" title="1.机器学习篇"></a>1.机器学习篇</h2><blockquote><p>机器学习片篇基本上按照李航老师和Andrew Ng 的顺序写分类和聚类的算法</p><ul><li>1.监督学习篇<ul><li>1.KNN</li><li>2.logistic regession</li></ul></li><li>2.非监督学习篇</li></ul></blockquote><h2 id="2-深度学习"><a href="#2-深度学习" class="headerlink" title="2.深度学习"></a>2.深度学习</h2><ul><li>1.卷积神经网络</li><li>2.递归神经网络</li></ul><h2 id="3-NLP篇"><a href="#3-NLP篇" class="headerlink" title="3.NLP篇"></a>3.NLP篇</h2><ul><li>1.概率图模型</li><li>2.汉语分词</li><li>3.句法分析</li><li>4.语义分析</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;最近在学习AI相关的知识，学了这么长时间总感觉前面的东西又忘了，而且我觉得只要写不出来的算法都是没有搞懂的，所以从今天开始写博客,复习学过的AI相关的知识，大概先从以下三个方向写：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1-机器学习篇
      
    
    </summary>
    
      <category term="目录" scheme="http://yoursite.com/categories/%E7%9B%AE%E5%BD%95/"/>
    
    
      <category term="menu" scheme="http://yoursite.com/tags/menu/"/>
    
  </entry>
  
</feed>
